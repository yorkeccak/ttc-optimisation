{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Reasoning Trace Generator  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to automatically create synthetic data points to add RAG tokens to the reasoning of Large Reasoning Models (LRMs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Environment Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import all required libraries in this section to ensure the rest of the notebook runs smoothly and reproducibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import os \n",
    "import re\n",
    "import json \n",
    "import requests \n",
    "from tqdm import tqdm \n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from openai.types import ChatCompletion\n",
    "\n",
    "# Environment Setup \n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_SK = os.getenv(\"OPENAI_SK\")\n",
    "DEEPSEEK_SK = os.getenv(\"DEEPSEEK_SK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines the utility functions used throughout the notebook to keep the main workflow clean and modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeChatRequestOpenAI(prompt: str, system_prompt: str, model: str = \"gpt-4o-mini-2024-07-18\", temperature: float = 0.7) -> str:\n",
    "    \"\"\"\n",
    "    Sends a chat completion request to the OpenAI API using the specified model and prompts.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user message or task prompt.\n",
    "        system_prompt (str): The system message that sets the behavior or context for the model.\n",
    "        model (str, optional): The OpenAI model to use. Defaults to \"gpt-4o-mini-2024-07-18\".\n",
    "        temperature (float, optional): Sampling temperature for response creativity. Defaults to 0.7.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise the OpenAI client with the OpenAI API key \n",
    "    client = OpenAI(api_key=OPENAI_SK)\n",
    "\n",
    "    # Send a chat completion request to the specified model with system and user messages\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def makeChatRequestDeepSeek(prompt: str, system_prompt: str, model: str = \"deepseek-reasoner\", temperature: float = 0.7) -> ChatCompletion:\n",
    "    \"\"\"\n",
    "    Sends a chat completion request to the DeepSeek-R1 model via the OpenAI-compatible API.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user's message or question to be sent to the model.\n",
    "        system_prompt (str): A message that sets the assistant's behavior or role.\n",
    "        model (str, optional): The identifier of the model to use. Defaults to \"deepseek-reasoner\".\n",
    "        temperature (float, optional): Sampling temperature that controls the randomness of output. \n",
    "                                       Lower values make output more deterministic. Defaults to 0.7.\n",
    "\n",
    "    Returns:\n",
    "        ChatCompletion: The raw response object returned by the DeepSeek API, containing model output,\n",
    "                        reasoning content, token usage, and other metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise the OpenAI client with the DeepSeek API key and base URL\n",
    "    client = OpenAI(api_key = DEEPSEEK_SK, base_url = \"https://api.deepseek.com\")\n",
    "\n",
    "    # Send a chat completion request to the specified model with system and user messages\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=temperature, \n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    return response \n",
    "    \n",
    "\n",
    "def generateCoT(question: str) -> tuple[str, str]: \n",
    "    \"\"\"\n",
    "    Generates a Chain-of-Thought (CoT) reasoning trace for a given question using the DeepSeek-R1 model.\n",
    "\n",
    "    Args:\n",
    "        question (str): The input question to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, str]: A tuple containing the model's main answer (`content`) and its reasoning trace (`reasoning_content`).\n",
    "    \"\"\"\n",
    "\n",
    "    # Set a general system-level prompt so that the LRM answers the question as it normally would \n",
    "    system_prompt = \"You are a helpful AI assistant.\"\n",
    "     \n",
    "    # Make a chat request to the DeepSeek-R1 model\n",
    "    response = makeChatRequestDeepSeek(question, system_prompt)\n",
    "\n",
    "    # Extract just the reasoning trace of this model and return it \n",
    "    return response.choices[0].message.content, response.choices[0].message.reasoning_content \n",
    "\n",
    "\n",
    "def updateCoTRAG(question: str, cot: str) -> str: \n",
    "    \"\"\"\n",
    "    Revises a given Chain of Thought (CoT) to include explicit calls to external retrieval \n",
    "    whenever external information is likely needed to justify reasoning steps.\n",
    "\n",
    "    Args:\n",
    "        question (str): The input question being reasoned through.\n",
    "        cot (str): The original Chain of Thought for the given question.\n",
    "\n",
    "    Returns:\n",
    "        str: The revised Chain of Thought, including embedded retrieval prompts and simulated results.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"You are an AI assistant specializing in synthetic data generation. Given original data, you can modify it to create a synthetic version by applying perturbations exactly as specified in the provided instructions.\"\n",
    "    prompt = f\"\"\" \n",
    "    Given a question and its corresponding Chain of Thought, revise the reasoning to include explicit calls to external retrieval **whenever needed**. The revised Chain of Thought should remain fluent, natural, and faithful to the original logic and answer, while clearly reflecting moments where external information is required.\n",
    "\n",
    "    The model should **not assume it has seen the content before**. Even if a term or concept appears straightforward, the model should verify or clarify it through retrieval if it is not broadly known, general-purpose knowledge. Internal knowledge should only be used when the information is clearly common-sense, canonical, or widely taught.\n",
    "\n",
    "    Retrieval should be triggered when the reasoning depends on:\n",
    "\n",
    "    - Obscure or specialized terms, concepts, or entities\n",
    "    - Background knowledge or definitions not guaranteed to be in the model’s pretraining\n",
    "    - Facts that are time-sensitive, dynamic, or uncertain\n",
    "    - Multi-hop questions that require chaining together facts\n",
    "\n",
    "    When retrieval is needed, the model should **briefly explain why**, then issue a search using the following tokenized format:\n",
    "\n",
    "    <search_query> your search query here </search_query>\n",
    "\n",
    "    <search_result> simulated search result here </search_result>\n",
    "\n",
    "    Multiple retrievals may be included in a single Chain of Thought, including multiple lookups for the same topic (e.g., clarification followed by fact checking). The model should incorporate each result naturally into the reasoning process.\n",
    "\n",
    "    The final Chain of Thought should remain coherent and lead to the original answer. Output only the revised Chain of Thought and nothing else.\n",
    "\n",
    "    Here are a few such examples:\n",
    "    1. \n",
    "    Question: What was the annual inflation rate in the U.S. for January 2025, and how does it compare to the rate in December 2024?\n",
    "    Chain of Thought: Okay, so I need to figure out what the annual inflation rate was in the U.S. for January 2025 and compare it to December 2024. Inflation rates are usually reported monthly by the Bureau of Labor Statistics (BLS) in the U.S., and they typically release these figures after the month has ended. \\n\\nSince today's date is March 7, 2025, the data for January 2025 should already be available on the BLS website. Looking at the reported figures, the annual inflation rate in December 2024 was 2.9%, while the annual inflation rate in January 2025 was 3.0%. This means that inflation saw a slight increase of 0.1% from December to January. \\n\\nIn summary, the U.S. inflation rate in January 2025 was 3.0%, up from 2.9% in December 2024. This indicates a slight uptick, possibly due to various economic factors affecting prices during that period.\n",
    "    \n",
    "    Output: \"To determine the annual inflation rate in the U.S. for January 2025 and compare it to December 2024, we need the most recent data published by the Bureau of Labor Statistics (BLS). Since inflation rates are updated monthly and it's currently March 2025, the relevant figures should be available. <search_query> U.S. annual inflation rate January 2025 site:bls.gov </search_query> <search_result> According to the Bureau of Labor Statistics, the annual inflation rate for the U.S. in January 2025 was 3.0%. In comparison, the rate for December 2024 was 2.9%. </search_result> The annual inflation rate in the U.S. for January 2025 was 3.0%, which is a slight increase from the 2.9% recorded in December 2024. This indicates a modest upward trend in inflation between the two months.\"\n",
    "    \n",
    "    2. \n",
    "    Question: A 45-year-old man presents with symptoms including a wide-based gait, a blank facial expression, hallucinations, memory issues, a resting tremor that resolves with movement, and bradykinesia. Based on these clinical findings, what is most likely to be observed in the histological specimen of his brain?\n",
    "    Chain of Thought: Okay, so here's a 45-year-old guy who's experiencing some pretty concerning symptoms. Let's break this down.\\n\\nHe's got a wide-based gait. Right off the bat, that makes me think about balance and coordination issues; something affecting his motor control could be at play here.\\n\\nThen there's the blank facial expression. That's often referred to as facial masking, commonly seen in Parkinson's disease due to a lack of facial muscle movement.\\n\\nHallucinations and memory issues, though, those are interesting. Hallucinations make me think about some sort of cognitive or psychiatric involvement. Memory issues definitely point towards a possibility of cognitive decline.\\n\\nNow, a resting tremor that stops with movement is a classic sign of Parkinson's. It's one of those distinguishing features. Paired with bradykinesia (slowness of movement), these two symptoms are screaming Parkinson’s disease to me.\\n\\nThe wide-based gait might initially make one consider cerebellar issues, but in the context of Parkinsonism, it can also be due to balance problems stemming from basal ganglia dysfunction or even Parkinson's-related gait disturbances.\\n\\nWhen I think about all these symptoms coming together, it certainly makes Parkinson's disease a strong contender, especially considering his age and the symptom profile.\\n\\nBut here's the thing—I can't ignore the cognitive and psychiatric symptoms. These could suggest something like Lewy body dementia (LBD), which is related but incorporates more psychiatric features alongside Parkinsonian symptoms.\\n\\nCellular Level Insight:\\nParkinson's disease and dementia with Lewy bodies share a common pathological hallmark: the presence of Lewy bodies. These are clumps of proteins, mainly alpha-synuclein, that accumulate in brain cells and cause both movement and cognitive problems.\\n\\nFinal Thought:\\nSo, taking all of this into account, when they look at the brain tissue under a microscope, I would expect them to find Lewy bodies. Those would explain the whole picture—movement issues, cognitive decline, and hallucinations. It ties everything together nicely.\n",
    "\n",
    "    Output: Okay, so here's a 45-year-old man with a mix of motor, cognitive, and psychiatric symptoms. Let's break it down. He has a wide-based gait, which often suggests balance and coordination problems. That could point to cerebellar dysfunction, but it can also be seen in Parkinsonian syndromes due to postural instability. Next, the blank facial expression—that’s facial masking, which is classic in Parkinson’s disease, caused by decreased facial muscle movement. Then we have hallucinations and memory issues. These aren’t as common in early Parkinson’s disease but are more characteristic of cognitive syndromes like Lewy body dementia. The presence of hallucinations, in particular, suggests we might be dealing with more than just a straightforward Parkinson’s diagnosis. I need to check whether early-onset hallucinations are more typical of Lewy body dementia than Parkinson’s. <search_query> early hallucinations and cognitive decline in Lewy body dementia vs Parkinson's disease </search_query> <search_result> In dementia with Lewy bodies (DLB), visual hallucinations and cognitive impairment tend to occur early and are core diagnostic features. In Parkinson's disease dementia, these symptoms typically develop later in the disease course. </search_result> Okay, so based on that, the early presence of hallucinations and memory problems in this patient fits better with dementia with Lewy bodies (DLB) than Parkinson’s disease. That helps clarify the diagnostic direction. Now, the resting tremor that resolves with movement and bradykinesia are hallmark features of Parkinsonian syndromes. But since DLB includes Parkinsonian motor symptoms, this still fits. So now, what’s going on at the cellular level? To answer that, I need to confirm what pathological finding is shared between Parkinson’s disease and Lewy body dementia, since the diagnosis could fall into either category. <search_query> histological hallmark of Lewy body dementia and Parkinson's disease </search_query> <search_result> Both Parkinson’s disease and dementia with Lewy bodies are characterized by the presence of Lewy bodies—intracellular aggregates composed mainly of alpha-synuclein. These are found in neurons throughout the brain, particularly in the substantia nigra and cortex. </search_result> Perfect. So both conditions share the presence of Lewy bodies, which are made up of alpha-synuclein aggregates in neurons. This aligns well with the clinical presentation of combined cognitive and motor symptoms. Final Thought: Given the patient's combination of early hallucinations, memory issues, and Parkinsonian motor signs, the most likely diagnosis is dementia with Lewy bodies. Therefore, on histological examination of the brain, we would expect to see Lewy bodies—intracellular inclusions composed primarily of alpha-synuclein.\n",
    "\n",
    "    Using these examples as references and keeping the detailed instructions in mind, refine and update the following Chain of Thought: \n",
    "    Question: {question}\n",
    "    Chain of Thought: {cot}\n",
    "    \"\"\"\n",
    "\n",
    "    return makeChatRequestOpenAI(prompt, system_prompt)\n",
    "\n",
    "\n",
    "def hasUnclosedSearchResult(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks whether the input text contains unclosed <search_result> tags.\n",
    "\n",
    "    This function scans the text for properly matched <search_result> and \n",
    "    </search_result> tags. It returns True if there are unmatched opening or \n",
    "    closing tags, and False otherwise.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to check for unclosed search result tags.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if unclosed or mismatched tags are found, False if all tags are properly closed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compile a regex pattern to match either opening or closing search result tags\n",
    "    pattern = re.compile(r'</?end?_?search_result>')\n",
    "\n",
    "    # Initialise a stack to keep track of opening tags\n",
    "    stack = []\n",
    "\n",
    "    # Iterate over each tag match in the text\n",
    "    for match in pattern.finditer(text):\n",
    "        tag = match.group()\n",
    "\n",
    "        # If we find an opening tag, push it onto the stack\n",
    "        if tag == '<search_result>':\n",
    "            stack.append(tag)\n",
    "\n",
    "        # If we find a closing tag\n",
    "        elif tag == '</search_result>':\n",
    "            # If there’s no matching opening tag on the stack, it's an error\n",
    "            if not stack:\n",
    "                return True  # closing tag with no corresponding opening\n",
    "            \n",
    "            # Otherwise, pop the corresponding opening tag\n",
    "            stack.pop()\n",
    "            \n",
    "    # After processing, if the stack isn’t empty, there are unmatched opening tags\n",
    "    return len(stack) > 0  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we download and preprocess the datasets that will serve as the foundation for synthetic sample generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise variable to denote which dataset you're using for synthesis \n",
    "dataset_no = 0\n",
    "dataset_lower_bound = 0\n",
    "dataset_upper_bound = 500\n",
    "dataset_names = [\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"cais/mmlu\"] \n",
    "\n",
    "if dataset_no == 0:\n",
    "    # Download the dataset from HuggingFace\n",
    "    dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
    "\n",
    "    # Get samples within the specified range \n",
    "    dataset_train = dataset[\"train\"].select(range(dataset_lower_bound, dataset_upper_bound))\n",
    "\n",
    "elif dataset_no == 1: \n",
    "    # Download the dataset from HuggingFace \n",
    "    dataset = load_dataset(\"cais/mmlu\", \"all\")\n",
    "\n",
    "    # Define the list of subjects we want to keep \n",
    "    allowed_subjects = [\"college_physics\", \"college_computer_science\", \"college_medicine\", \"high_school_government_and_politics\", \"high_school_macroeconomics\", \"high_school_microeconomics\", \"elementary_mathematics\", \"high_school_european_history\", \"high_school_computer_science\", \"high_school_chemistry\", \"high_school_biology\", \"high_school_geography\", \"high_school_microeconomics\", \"high_school_us_history\", \"anatomy\", \"astronomy\", \"international_law\", \"machine_learning\", \"nutrition\", \"prehistory\", \"sociology\", \"us_foreign_policy\", \"computer_security\"]\n",
    "\n",
    "    # Filter the dataset to only include rows with subjects in the allowed list\n",
    "    dataset_filtered = dataset[\"test\"].filter(lambda example: example[\"subject\"] in allowed_subjects)\n",
    "\n",
    "    # Get samples within the specified range\n",
    "    dataset_train = dataset_filtered.select(range(dataset_lower_bound, dataset_upper_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation Loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-RAG Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script presented here generates reasoning traces that answer questions without the use of any RAG tokens to access external information. We use the `deepseek-reasoner` model, which corresponds to `DeepSeek-R1:671B`, and directly extract its reasoning trace as the final trace—without applying any further modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store synthetic samples\n",
    "synthetic_dataset = []\n",
    "\n",
    "# Initialise a string to keep track of the indices at which errors occur during synthesis\n",
    "error_str = \"\"\n",
    "\n",
    "print(\"Beginning synthesis process...\")\n",
    "for i in tqdm(range(len(dataset_train)), desc=\"Synthesizing samples\"):\n",
    "    # Extract the current sample from the dataset \n",
    "    sample = dataset_train[i]\n",
    "\n",
    "     # Generate a Chain of Thought and response for the current question\n",
    "    question = sample[\"question\"] + \"Options: \" + str(sample[\"choices\"])\n",
    "\n",
    "    # Try to generate Chain of Thought with unlimited retries \n",
    "    retry_count = 0\n",
    "    success = False  \n",
    "\n",
    "    while not success: \n",
    "        try: \n",
    "            response, cot = generateCoT(question)\n",
    "            success = True \n",
    "        except Exception as e: \n",
    "            retry_count += 1\n",
    "            print(f\"Retry #{retry_count} for sample {i + dataset_lower_bound}. Error: {str(e)}\")\n",
    "         \n",
    " \n",
    "    # Create a dictionary to create a synthetic sample\n",
    "    sample_dict = {\n",
    "        \"Question\": question,\n",
    "        \"CoT\": cot,\n",
    "        \"Response\": response\n",
    "    }\n",
    "\n",
    "    # Store the synthetic sample in a list  \n",
    "    synthetic_dataset.append(sample_dict)\n",
    "    \n",
    "print(\"Synthesis process complete!\")\n",
    "\n",
    "# Write to JSON file\n",
    "print(\"Writing synthetic samples to JSON...\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "sanitised_dataset_name = dataset_names[dataset_no].replace('/', '_')\n",
    "with open(f\"synthetic_dataset_{sanitised_dataset_name}_{dataset_lower_bound}:{dataset_upper_bound}_{error_str}_{timestamp}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(synthetic_dataset, f, ensure_ascii=False, indent=4)\n",
    "print(\"Writing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script generates synthetic samples that demonstrate the use of RAG tokens to retrieve external information. Since the dataset we have selected for pertubation already contains reasoning traces, we only need to update them with our engineered prompt. This modification is handled by the `gpt-4o-mini-2024-07-18` model, which we found to strike a good balance between instruction-following capability and cost-efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning synthesis process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Synthesizing samples: 100%|██████████| 500/500 [1:17:29<00:00,  9.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthesis process complete!\n",
      "Writing synthetic samples to JSON...\n",
      "Writing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialise a list to store synthetic samples\n",
    "synthetic_dataset = []\n",
    "\n",
    "print(\"Beginning synthesis process...\")\n",
    "for i in tqdm(range(len(dataset_train)), desc=\"Synthesizing samples\"):\n",
    "    # Extract the current sample from the dataset \n",
    "    sample = dataset_train[i]\n",
    "\n",
    "    # Extract the question, Chain of Thought, and response from the current sample \n",
    "    question = sample[\"Question\"]\n",
    "    cot = sample[\"Complex_CoT\"]\n",
    "    response = sample[\"Response\"]\n",
    "\n",
    "    # Try to update Chain of Thought with unlimited retries \n",
    "    retry_count = 0\n",
    "    success = False \n",
    "\n",
    "    while not success: \n",
    "        try: \n",
    "            # Update the Chain of Thought of the original sample \n",
    "            updated_cot = updateCoTRAG(question, cot)\n",
    "            success = True \n",
    "        except Exception as e: \n",
    "            retry_count += 1\n",
    "            print(f\"Retry #{retry_count} for sample {i + dataset_lower_bound}. Error: {str(e)}\")\n",
    "\n",
    "    # Create a dictionary to create a synthetic sample\n",
    "    sample_dict = {\n",
    "        \"Question\": question,\n",
    "        \"CoT\": updated_cot,\n",
    "        \"Response\": response\n",
    "    }\n",
    "\n",
    "    # Store the synthetic sample in a list  \n",
    "    synthetic_dataset.append(sample_dict)\n",
    "print(\"Synthesis process complete!\")\n",
    "\n",
    "# Write to JSON file\n",
    "print(\"Writing synthetic samples to JSON...\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "sanitised_dataset_name = dataset_names[dataset_no].replace('/', '_')\n",
    "with open(f\"synthetic_dataset_{sanitised_dataset_name}_{dataset_lower_bound}:{dataset_upper_bound}_{timestamp}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(synthetic_dataset, f, ensure_ascii=False, indent=4)\n",
    "print(\"Writing complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below detects and addresses improperly closed tags in the dataset. During fine-tuning, we found that even a few malformed examples could significantly affect results. To avoid this, we use the script to manually identify and fix incomplete tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "with open('dataset.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Scan dataset\n",
    "bad_entries = []\n",
    "for i, entry in enumerate(data):\n",
    "    if 'CoT' in entry and hasUnclosedSearchResult(entry['CoT']):\n",
    "        bad_entries.append((i, entry['Question']))\n",
    "\n",
    "# Report\n",
    "if bad_entries:\n",
    "    print(f\"Found {len(bad_entries)} entries with mismatched <search_result> / </search_result> tags:\")\n",
    "    for idx, question in bad_entries:\n",
    "        print(f\" - Entry #{idx}: {question}\")\n",
    "else:\n",
    "    print(\"All entries have properly matched <search_result> and </search_result> tags.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
